![hero](banner.png)

<p align="center">
	<h1 align="center"><b>model.eval</b></h1>
<p align="center">
    Run your business smarter
    <br />
    <br />
    <a href="https://github.com/danielft2/model_eval/issues">Issues</a>
  </p>
</p>

## Sobre model.eval

model.eval √© uma plataforma desenvolvida como parte do meu Trabalho de Conclus√£o de Curso (TCC) na Universidade Federal do Cear√°. A plataforma oferece um ambiente voltado para pesquisadores que desejam avaliar a performance de seus Grandes Modelos de Linguagem (LLMs) na gera√ß√£o de quest√µes educacionais utilizando avalia√ß√µes autom√°ticas e humanas.

Inicialmente, a model.eval foca na avalia√ß√£o da qualidade das quest√µes educacionais geradas por modelos derivados do T5, especificamente contexto de quest√µes atreladas a Base Nacional Comum Curricular (BNCC). 

## Funcionalidades

### Avalia√ß√£o Autom√°tica  
- Configure e gerencie avalia√ß√µes com m√©tricas autom√°ticas.  
- Envie seu conjunto de teste para avaliar o modelo.  
- Execute avalia√ß√µes em modelos configurados e obtenha resultados de m√©tricas autom√°ticas.  

### üìù Avalia√ß√£o Humana  
- Configure avalia√ß√µes baseadas em feedback humano.  
- Personalize as avalia√ß√µes conforme suas necessidades.  
- Importe as quest√µes geradas pelo modelos para que sejam avaliadas.  
- Acompanhe os resultados de forma geral, por quest√£o ou por descritor.  
- Gere links de compartilhamento para que avaliadores acessem o formul√°rio de avalia√ß√£o.  

## Get started

We are working on the documentation to get started with Midday for local development: https://docs.midday.ai

## App Architecture

- React
- TypeScript
- Nextjs
- Supabase
- Shadcn
- TailwindCSS

### Hosting

- Supabase (storage)
- Vercel (Website, edge-config, and metrics)
