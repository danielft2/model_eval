![hero](banner.png)

<p align="center">
	<h1 align="center"><b>model.eval</b></h1>
<p align="center">
    Run your business smarter
    <br />
    <br />
    <a href="https://github.com/danielft2/model_eval/issues">Issues</a>
  </p>
</p>

## Sobre model.eval

model.eval é uma plataforma desenvolvida como parte do meu Trabalho de Conclusão de Curso (TCC) na Universidade Federal do Ceará. A plataforma oferece um ambiente voltado para pesquisadores que desejam avaliar a performance de seus Grandes Modelos de Linguagem (LLMs) na geração de questões educacionais utilizando avaliações automáticas e humanas.

Inicialmente, a model.eval foca na avaliação da qualidade das questões educacionais geradas por modelos derivados do T5, especificamente contexto de questões atreladas a Base Nacional Comum Curricular (BNCC). 

## Funcionalidades

### Avaliação Automática  
- Configure e gerencie avaliações com métricas automáticas.  
- Envie seu conjunto de teste para avaliar o modelo.  
- Execute avaliações em modelos configurados e obtenha resultados de métricas automáticas.  

### Avaliação Humana  
- Configure avaliações baseadas em feedback humano.  
- Personalize as avaliações conforme suas necessidades.  
- Importe as questões geradas pelo modelos para que sejam avaliadas.  
- Acompanhe os resultados de forma geral, por questão ou por descritor.  
- Gere links de compartilhamento para que avaliadores acessem o formulário de avaliação.  

## Get started

We are working on the documentation to get started with Midday for local development: https://docs.midday.ai

## App Architecture

- React
- TypeScript
- Nextjs
- Supabase
- Shadcn
- TailwindCSS

### Hosting

- Supabase (storage)
- Vercel (Website, edge-config, and metrics)
